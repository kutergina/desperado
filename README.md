"# desperado" 


Алгоритм

0' поместить начальную страницу в очередь

1 выбрать из очереди ресурс (для первой итерации точка входа)
с пунктом 0' не нужно дополнительных условий на первую/не первую итерацию

1' Проверить robots можно здесь. Думаю, логичнее делать это непосредственно
перед запросом страницы. Или во всяком случае, после того, как ссылка пройдет
все остальные фильтры (радиус и т.д.) и, возможно, сохранена. И первую ссылку
тоже нужно проверять.

2 скачать страницу

3 выбрать все ссылки

4 проверить все ссылки с robot.txt
См. 1'

4' Где-то здесь их нужно сохранить. Cсылки, которые мы сохраняем - не
обязательно те же ссылки, которые мы собираемся краулить.
	
5 фильтр ссылок по своему алгоритму (удалить повторы и, например, не нужные по какому-то признаку, ну или еще что)

6 все, что осталось от списка ссылок, занести в очередь	

7 шаг 1	и так до тех пор, пока не достигнем нужной глубины (радиуса)
пока очередь не опустеет


Все ли так?


Вопросы

1 на каком этапе мы представляемся, если оно надо - всегда ли надо (User_Agent?)
когда скачиваем страницу или robots, т.е. всегда - мы вежливые и послушные

2 алгоритм у зверушки какой (просто пербор ссылок для начала или еще какой критерий?)
Для начала - радиус и есть наш алгоритм, т.е. краулем все, что в пределах
радиуса от начальной страницы. Можно и еще что-нибудь придумать.

3 где хранить очередь (память, бд, файл) и в каком виде (перечень, струтура)
очередь - в памяти в любом удобном виде
найденные ссылки, по-моему, достаточно просто в текстовый файл записать в 
как список пар (source, target)

4 если очень много ссылок на странице, нужно ли ограничивать по количеству? или ввести ограничение по аргументу (как радиус, таймаут, но по умолчанию какую-то цифру отпределить)?
У нас же есть уже ограничение на размер страницы. Думаю, этого достаточно.

5 нужна ли какая-нибудь сортировка ссылок или как обходит робот, так пусть и будет?
Тех, что сохраняем? Нет, зачем? Потом все обработаем отдельно, там где понадобится.

6 по идее, чтобы не запутаться в глубине, нам как-то нужно сразу обработать все ссылки из очереди (многопоточность?) или устроить хитрую работу со списками (но можно в них тоже запутаться)
Лучше обойтись без многопоточности: в нашем случае производительности это
не добавит, а проблем - да. (Постараемся не связываться с большим числом ссылок).
Посмотри поиск в ширину в графе - наш краулер может делать что-то вроде этого.
