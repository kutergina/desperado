"# desperado" 


Алгоритм

1 выбрать из очереди ресурс (для первой итерации точка входа)

2 скачать страницу

3 выбрать все ссылки
	
4 проверить все ссылки с robot.txt
	
5 фильтр ссылок по своему алгоритму (удалить повторы и, например, не нужные по какому-то признаку, ну или еще что)

6 все, что осталось от списка ссылок, занести в очередь	

7 шаг 1	и так до тех пор, пока не достигнем нужной глубины (радиуса)


Все ли так?


Вопросы

1 на каком этапе мы представляемся, если оно надо - всегда ли надо (User_Agent?)

2 алгоритм у зверушки какой (просто пербор ссылок для начала или еще какой критерий?)

3 где хранить очередь (память, бд, файл) и в каком виде (перечень, струтура)

4 если очень много ссылок на странице, нужно ли ограничивать по количеству? или ввести ограничение по аргументу (как радиус, таймаут, но по умолчанию какую-то цифру отпределить)?

5 нужна ли какая-нибудь сортировка ссылок или как обходит робот, так пусть и будет?

6 по идее, чтобы не запутаться в глубине, нам как-то нужно сразу обработать все ссылки из очереди (многопоточность?) или устроить хитрую работу со списками (но можно в них тоже запутаться)
